\chapter{Grundlagen}

In diesem Kapitel werden die Grundlagen der verwendeten Themen und Technologien --- Begriffe und Sachverhalte --- erläutert, die für das Verständnis der folgenden Arbeit notwendig sind.
Die Grundlagen werden ohne Bezug auf das konkrete Projekt, allgemein dargestellt. Sie sind zielgerichtet auf das Verständnis des Hauptteils bezogen.

Der Abschnitt zur Mathematik dient sowohl zur Einführung eventuell neuer Konzepte sowie zur Definition der in dieser Arbeit verwendeten Nomenklatur von bereits Bekanntem.

% \paragraphnl ist ein custom command. Ein \paragraph plus ein line-break / new-line.
% (Nach \paragraphnl{Paragraph} muss eine Zeile frei gelassen werden.)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Mathematics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mathematik}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wahrscheinlichkeitsdichtefunktionen}
Eine Wahrscheinlichkeitsdichtefuntion (WDF) ist eine reelle Funktion $p$, welche die Wahrscheinlichkeit beschreibt, dass eine Zufallsvariable X in einem bestimmten Bereich liegt. Diese Wahrscheinlichkeit wird wie folgt berechnet:
\begin{equation}
    a,b \in X \land a \le b, \space p([a, b]) = \int_a^b p(x) dx
\end{equation}
Die WDF hat dabei folgende zwei Invarianten zu erfüllen\footnote{
    Deisenroth: Mathematics for Machine Learning, S. 181
    \cite{Deisenroth2020}
}:
\begin{enumerate}
    \item Sie ist über den gesamten Definitionsbereich nicht negativ. 
    \begin{equation}
        \forall x \in X, \space p(x) \ge 0
    \end{equation}
    \item Das Integral der WDF-Funtion ist berechenbar und über den Definitionsbereich gleich eins.
    \begin{equation}
        \int p(x) dx = 1
    \end{equation}
\end{enumerate}

Oft kann es sein, dass die Parametrisierung angegeben werden soll. Dies wird im weiteren Verlauf dieser Arbeit als $p_\theta(X)$ denotiert, wobei in diesem Fall $\theta$ die Parametrisierung ist. Der Vollständigkeithalber sei erwähnt, dass einige Publikationen eine alternative Schreibweise $p(X; \theta)$ wählen. \\
Desweiteren wird im Bereich des Machine Learning oft die Entnahme einer Stichprobe $x$ aus einer WDF $p$, was auch Sampling gennant wird, $x \sim p$ gekennzeichnet. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multidimensionale gaußsche Normalverteilung}

Die gaußsche Normalverteilung $\mathcal N$, oder einfach Normalverteilung ist eine der bekanntesten kontinuierlichen Wahrscheinlichkeitsverteilungen. Definiert ist sie im mehrdimensionalen Fall mit $d$ Dimensionen durch ihre WDF:
\begin{equation}
    \mathcal N(x; \mu, \Sigma) = 
    \frac{1}{\sqrt{(2\pi)^d \det{\Sigma}}}
    e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)}
\end{equation}
$\mu$ ist hierbei der Erwahrtungswert und $\Sigma$ die Kovarianzmatrix. \\
Eine besondere Variante ist hierbei die Standardnormalverteilung $\mathcal N(x; 0, I)$ welche oft einfach durch $\mathcal N$ abgekürzt wird.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Erwartungswert}

Der Erwartungswert einer Zufallsvariable $X$ unter einer WDF $p(X)$ ist gegeben als: 
\begin{equation}
    \mathbb E(X) := \int x p(x) dx
\end{equation}
In vielen Publikationen im Bereich der künstlichen Intelligenz wird der Erwartungswert auch wie folgt angegeben:
\begin{equation}
    \mathbb E_{x \sim p}[x] := \int x p(x) dx
\end{equation}
Diese Definition kann ebenfalls um den Erwartungswert einer Funktion $f(x)$ angewandt werden, wenn x aus der Wahrscheinlichkeitsverteilung $p(X)$ entnommen wird. 
\begin{equation}
    \mathbb E_{x \sim p}[f(x)] := \int f(x) p(x) dx
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood}

Die Likelihood $p(y|x)$ ist in der Bayesischen Statistik, vereinfacht ausgedrückt, eine Aussage über die Wahrscheinlichkeit, dass die Beobachtung $x$ der WDF $p$ entnommen wurde\footnote{
    Deisenroth: Mathematics for Machine Learning, S. 185
    \cite{Deisenroth2020}
}. \\
Im bereich des Machine Learning ist es oft das Ziel, die Likelihood einer parametrisierten WDF zu maximieren\footnote{
    Vgl. Goodfellow, Bengio, Courville: Deep Learning, S. 133
    \cite{Goodfellow-et-al-2016}
}. 
\begin{equation}
    \arg\max_\theta p_\theta(y|x)
\end{equation}'
Dies entspricht sinngemäß der parametrisierung der WDF, unter welcher, die beobachteten Daten, beziehungsweise die Trainingsdaten maximal wahrscheinlich sind. \\
Diese Maximierung entspricht $\arg\max_\theta \log p_\theta(y|x)$ welche auch als die Maximierung der \textit{Log-Likelihood} bezeichnet wird. Diese Formulierung wird der einfachen Likelihood in der Praxis oft vorgezogen, da sie bei der Differenzierung Vorteile hat.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Score}

Der \textit{score} $s$ ist der Gradient, also der Vektor aller partiellen Ableitungen der Log-Likelihood einer WDF $p$\footnote{
    Hyvärinen: Estimation of Non-Normalized Statistical Models by Score Matching, S. 2
    \cite{JMLR:v6:hyvarinen05a}
}: 
\begin{equation}
    s_\theta(x) = \frac{\partial \log p_\theta(x)}{\partial \theta} 
    = \nabla_x p_\theta(x)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Kullback-Leibler-Divergenz}

Seien $p(x)$ und $q(x)$ zwei WDFs, so ist die KL-Divergenz definiert als\footnote{
    Vgl. Goodfellow, Bengio, Courville: Deep Learning, S. 74 f.
    \cite{Goodfellow-et-al-2016}
}:

\begin{equation}
D_\text{KL}(p||q) 
:= \mathbb{E}_{x \sim p} \left [
\log \frac{p(x)} {q(x)}
\right ]
= \mathbb{E}_{x \sim p} \left [ 
    \log p(x) - \log q(x)
    \right ]
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Generative Künstliche Intelligenz}

Modelle der generativen KI versuchen im Allgemeinen die Wahrscheinlichkeitsverteilung der Eingabedaten, oft auch als Datenverteilung bezeichnet, zu modellieren. Sie unterscheiden sich somit von diskreminierenden Ansätzen, welche lediglich Eigenschaften dieser Verteilung erlernen. \\
Es kann schwierig fallen, intuitiv die Beziehung zwischen Datenverteilung und Modell nachzuvollziehen. Um diesen Sachverhalt zu verdeutlichen, folgt ein einfaches Beispiel:

Ein Münzwurf soll modelliert werden. Das Ergebnis von Münzwürfen ist mit der Wahrscheinlichkeitsverteilung $p(X)$ beschrieben wobei $x \in [\text{Kopf}, \text{Zahl}]$. Um zu verdeutlichen, dass es sich bei $p(X)$ um die Datenverteilung handelt wird sie oft auch als $p_\text{data}(X)$ bezeichnet. \\
Das Modell, welches den Münzwurf modellieren soll wird $p_\theta(X)$ bezeichnet, wobei $\theta$ die Parameter dieses Modells darstellt. Das Ziel ist, dass Münzwürfe des Modells, nicht von tatsächlichen Münzwürfen unterschieden werden können. Einfacher ausgedrückt, müssen die Wahrscheinlichkeiten für Kopf und Zahl jeweils gleich sein: $p_\theta(X) \overset{!}{=} p_\text{data}(X)$. \\ 
Um dies zielgerichtet erfüllen zu können benötigen wir eine Metrik, welche angibt, wie gut das Modell die Datenverteilung approximiert. Hierfür kann beispielsweise die Distanz zwischen beiden Verteilungen genommen werden. Eine Möglichkeit diese zu Berechnen ist die KL-Divergenz $D_\text{KL}(p_\text{Data}(X)||p_\theta(X))$. Um eine solche Berechnung durchzuführen muss $p_\text{Data}(X)$ allerdings bekannt sein. Grundsätzlich ist dies allerdings für Datenverteilungen nicht möglich. \\
Die Minimierung der KL-Divergenz ist allerdings mathematisch äquivalent zur minimierung Kreuzentropie\footnote{
    Goodfellow, Bengio, Courville: Deep Learning, S. 75
    \cite{Goodfellow-et-al-2016}
}. 
Dies entspricht wiederum der maximierung der Log-Likelihood und ist somit als Trainingskriterium bei Modellen bekannt.
Damit $p_\theta(X)$ $p_\text{data}(X)$ erlernen kann, werden zunächst Beispiele benötigt. Dazu wird mehrfach eine Münze geworfen. Die Menge all dieser Ergebnisse sind die Trainingsdaten. Ein einzelner Münzwurf $x_n$ der Menge entspricht einem Sample, welches der Datenverteilung entnommen wurde $x_n \sim p_\text{data}(X)$. Nun kann über die Maximierung der Log-Likelihood normales Training vollzogen werden.

Dieses Beispiel kann auf andere Anwendungsfälle übertragen werden. In der Bildysnthese entspricht die Datenverteilung beispielsweise einer Verteilung von Farbwerten auf $n \times m$ Pixeln, im Gegensatz zu einem Münzwurf auf Kopf oder Zahl.

% Autoencoder %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autoencoder}

Autoencoder (AE) sind streng genommen keine generativen Modelle, da sie lediglich rekonstruieren und keine neuen Daten erzeugen. Sie stellen allerdings die Grundlage für die Variational Autoencoder dar, welche generativ sind und im folgenden Unterabschnitt behandelt werden. \\ 
AEs haben als grundlegendes Ziel haben eine Eingabe in der dimensionalität zu reduzieren und anschließend wieder zur ursprünglichen Eingabe zu rekonstruieren. Man kann sie sich entsprechend als eine erlernte Variante von Kompressionsalgorithmen wie ZIP vorstellen. 
Zu diesem Zweck bestehen AEs aus zwei Komponenten\footnote{
    Die Encoder und Decoder Komponenten werden hier als $q$ und $p$ bezeichnet um die Verbindung zu Variational Autoencodern zu verdeutlichen. In vielen Publikationen werden sie allerdings $E$ und $D$ denotiert.    
}:
\begin{itemize}
    \item \textbf{Encoder $q_\phi(x)$}: \\
    Erhält als Eingabe Samples der Datenverteilung und komprimiert sie zu einem Code $h$.
    \item \textbf{Decoder $p_\theta(h)$}: \\
    Erhält als Eingabe den versteckten Zustand und rekonstruiert das ursprüngliche Sample der Datenverteilung möglichst verlustfrei. Diese Komponente modelliert die Datenverteilung auf basis des Codes.
\end{itemize}

Das Training eines solchen AEs ist sehr simpel. Man minimiere lediglich die Distanz zwischen Eingabedaten und Rekonstruktionen. Somit ergibt sich folgendes Optimierungsziel:

\begin{equation}
    \min L_{\phi, \theta}(x) = \mathbb E _{x \sim p_{data}} 
    \left [ 
        d(x, p_\theta(q_\phi(x)))
    \right ]
\end{equation}

% VAE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Variatonal Autoencoder}

Eine spezialisierte Variante der Autoencoder sind sogenannte Variational Autoencoder (VAE)\footnote{
    Kingma, Welling: Auto-Encoding Variational Bayes
    \cite{kingma2013auto}
}. Ihre Erweiterung der Autoencoder besteht in der Einführung einer Bedingung an den Code. Dieser kann in im ursprünglichen AE ein praktisch beliebiger Vektor sein, solange er bei der Entschlüsselung von Vorteil ist. Je nach Anwendungsfall kann es allerdings hilfreich sein, wenn der Code selbst auch eine semantisch Aussagekräftige Darstellung der Eingabe ist. Dies kann den Code, je nach Anwendungsfall, zu einem geringerdimensionalen Substitut für die Ursprungsdaten machen. Hierbei ist allerdings wichtig zu beachten, dass es sich hierbei in aller Regel nicht um menschenverständliche Eigenschaften handelt. \\
Zu diesem Zweck wird der latente Ergebnisraum des Encoders mit einer beliebigen Wahrscheinlichkeitsverteilung $p(z)$ kombiniert. Eingabedaten werden somit also einen latenten Raum $Z$ abgebildet, welcher grob $p(z)$ entspricht. \\
Die zugrundeliegende Aussage die somit getroffen wird ist, dass jede Komponente des Codes nicht nur eine Kompression der Eingabe ist, sondern auch selbst einer Wahrscheinlichkeitsverteilung folgt. Hierbei wird angenommen, dass Merkmale der Datenverteilung, die beispielsweise einer Normalverteilung folgen, eine strukturierte und nützliche Darstellung der Eingabedaten ermöglichen. Die so entstehende Verteilung hat die folgende Form:
\begin{equation}
    p_\theta(x) = \int_z p_\theta(x, z) dz
\end{equation}
Diese Anpassung hat zur Folge, dass das Training eines VAE ein neues Optimierungsziel benötigt. Der erste Gedanke ist, die Log-Likelihood der Verteilung $p_\theta(X)$ zu maximieren. Dies ist aufgrund des Integrals über den latenten Raum nicht effizient möglich. Dieses Problem wird gelöst, indem eine zweite Quantität maximiert wird, welche garantiert kleiner als die Log-Likelihood ist. Diese Quantität ist das Optimierungsziel und wird allgemein die \textit{Evidence Lower Bound} (ELBO) genannt. Sie ist gegeben als:
\begin{equation}
    \mathcal \max L_{\phi, \theta}(x) = \mathbb E_{z \sim q_{\phi}(z|x)}
    \left [
        \log p_\theta(x|z)
    \right ]
    - D_\text{KL} (\log q_{\phi}(z|x) || p(z))
\end{equation}
$p(z)$ ist hierbei die, für die Verteilung der Merkmale angenommene Verteilung. In der Regel handelt es sich hierbei um eine Standardnormalverteilung $\mathcal N(0; I)$. Die Herleitung der ELBO ausgehend von der Log-Likelihood ist sehr komplex und dem Verständnis nicht weiter zutragend. Eine Betrachtung der einzelnen Therme bietet allerdings Aufschluss über die Bedeutung der ELBO:
\begin{itemize}
    \item $\mathbb E_{z \sim q_{\phi}(z|x)}
        \left [
            \log p_\theta(x|z)
        \right ]$: \\
    Hierbei handelt es sich um die Log-Likelihood der Eingabedaten unter der Bedingung der jeweiligen latenten Repräsentation. Einfacher ausgedrückt stellt dies die Übereinstimmung der Eingabe und der Rekonstruktion sicher. Somit ist dies lediglich das Optimierungsziel des ursprünglichen AE.
    \item $-D_\text{KL} (\log q_{\phi}(z|x) || p(z))$: \\
    Diese KL-Divergenz stellt eine Regularisierungstherm dar. Sie fordert, dass die Verteilung $q_{\phi}(z|x)$, sprich der latente Ergebnisraum des Encoders, der angenommenen zugrundeliegenden Verteilung $p(z)$ entspricht.
\end{itemize}
Tatsächlich wurde also, gegenüber des ursprünglichen Optimierungsziels nur ein Regularisierungstherm hinzugefügt.



% GAN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generative Adversarial Networks}

Maximierung der Log-Likelihood ist nicht die einzige Methode eine Datenverteilung zu erlernen. Tatsächlich schränkt sie häufig die Komplexität der zugrundeliegenden Funktionen ein, da sie von Modellen fordert, dass sie als Wahrscheinlichkeitsverteilungen formuliert werden, welche somit die damit verbundenen mathematischen Invarianten erfüllen müssen. Dies kann die Qualität der Ergebnisse beeinträchtigen. Insbesondere bei Bilddaten kommt hinzu, dass die Log-Likelihood nicht unbedingt ein gutes Maß für visuelle Qualität ist.\\
Generative Adversarial Networks (GANs)\footnote{
    Goodfellow et al.: Generative Adversarial Nets
    \cite{goodfellow2014generativeadversarialnetworks}
} erlernen die Datenverteilung auf eine andere Weise. In ihnen kommen zwei Komponenten zum Einsatz der \textit{Diskriminator} und der \textit{Generator} welche zwei gegensätzliche Ziele verfolgen:
\begin{itemize}
    \item \textbf{Diskriminator $D_\phi$}: \\
    Der Diskriminator ist, wie der Name vermuten lässt, kein generatives Modell. Er hat als Ziel zwischen den Ausgaben des Generators und Samples der Datenverteilung zu unterscheiden.
    \item \textbf{Generator $G_\theta$}: \\
    Das Ziel des Generators ist das Erzeugen von Datenpunkten andhand eine einfachen Ausgangsverteilung $p_z$, die vom Diskriminator nicht von echten Daten unterschieden werden können. 
\end{itemize}
Die Idee hinter diesen gegeneinander gerichteten Komponenten ist, dass somit Daten generiert werden, die möglichst ähnlich zu realen Daten sind. Das Optimierungsziel entspricht somit dieser Form\footnote{
    Goodfellow et al.: Generative Adversarial Nets, S. 3
    \cite{goodfellow2014generativeadversarialnetworks}
}:
\begin{equation}
    \min_{G_\theta}\max_{D_\phi}V(G_\theta, D_\phi)
    = \mathbb E_{x \sim p_\text{data}}[\log D_\phi(x)] 
    + \mathbb E_{z \sim p_z}[\log (1 - D_\phi(G_\theta(z)))]
\end{equation}
Die Gegensätzlichkeit ist hier durch das Minimax ausgedrückt, bei welchem der $V$ respektiv zum Generator minimiert und zum Diskriminator maximiert wird. Hervorzuheben ist, dass der Generator über das Täuschen des Diskriminators traininiert wird. \\
Diese Herangehensweise ist zwar gut um Erzeugen von hochqualitativen Samples, allerdings erweist sie in der Praxis viele Probleme, die nur schwierig zu bewältigen sind. Ein prominentes dieser Probleme ist der sogenannte \textit{Mode Collapse}. Hierbei handelt es sich um einen Zustand, bei dem der Generator nur einen kleinen Bereich der Datenverteilung erlernt. Dies ist ein Umstand der auf das Trainingskriterium des Generators zurückzuführen ist. Hierbei ist, im Gegensatz zur Maximierung der Log-Likelihood, nicht sichergestellt, dass die gesamte Datenverteilung betrachtet wird. Es kann ausreichen nur einen kleinen bereich zu erlernen, welcher aber vom Diskriminator als real eingestuft wird.\footnote{
    Thanh-Tung, Tran: Mode Collapse in Generative Adversarial Networks, S. TODO
    \cite{thanhtung2020catastrophicforgettingmodecollapse}
}

\subsubsection{VAE-GAN}

Insbesondere bei Bilddaten können Log-Likelihood basierte Modelle qualitative Mängel aufweisen. Dies ist darin begründet das Optimierungsziele zum Optimieren der Log-Likelihood wie beispielsweise die Kreuzentropie nicht unbedingt gute Kriterien für visuelle Ähnlichkeit zur Datenverteilung sind. So kommt es bei VAEs unter anderem oft zu verwaschenen Rekonstruktionen. \\
VAE-GANs haben als Ziel, die visuelle Qualität von Ergebnissen von VAEs zu verbessern. Sie tun dies indem sie, wie ihr Name vermuten lässt, VAEs mit GANs kombinieren. Konkret wird hierbei der Decoder des VAEs als Generator eines GANs angesehen. Somit müssen die aus dem Decoder resultierenden Rekonstruktionen einen Diskriminator täuschen, welcher die visuelle Qualität sicherstellen soll. Das Trainingsziel ist hierbei ebenfalls eine Kombination beider Ansätze\footnote{
    Larsen et al.: Autoencoding using a learned similarity metric, S. 2
    \cite{larsen2016autoencoding}
}:
\begin{equation}
    L_\text{VAE-GAN}
    =  L_\text{prior} +  L_{D_l\text{ like}} +  L_\text{GAN}
\end{equation}
Wobei $ L_\text{prior}$ und $ L_\text{GAN}$ bereits vorgestellt wurden und $ L_{D_l\text{ like}}$ eine Abwandlung des ersten Therms der ELBO darstellt. Diese beruht nun auf den Aktivierungen der $l$-ten Schicht des Diskriminators des GAN. 
\begin{equation}
    L_\text{prior} 
    = D_\text{KL} (\log q_{\phi_\text{VAE}}(z|x) || p(z))
\end{equation}
\begin{equation}
    L_\text{GAN} = \mathbb E_{x \sim p_\text{data}}[\log D_{\phi_\text{GAN}}(x)] 
    + \mathbb E_{z \sim p_z}
    [\log (1 - D_{\phi_\text{GAN}}(G_{\theta_\text{GAN}}(z)))]
\end{equation}
\begin{equation}
    L_{D_l\text{ like}} = 
    - \mathbb E_{z \sim q_{\phi_\text{VAE}}(z|x)}
    \left [
        \log p_{\theta_\text{VAE}}(D_{l, \phi_\text{GAN}}(x)|z)
    \right ]
\end{equation}

In der Praxis wird allerdings oft anstelle von $L_{D_l\text{ like}}$ der ursprüngliche Rekonstruktionsfehler $- \mathbb E_{z \sim q_{\phi}(z|x)}
    \left [
        \log p_\theta(x|z)
    \right ]$ von VAEs angewandt. \\
Diese Variante von VAEs ist in der Lage deutlich ähnlicher scheinende Rekonstruktionen zu erzeugen, als es einfache VAEs können.


\subsubsection{Visual Similarity Metrics}



% SUPPORTING MODELS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{UNET}

\subsection{Transformer}


\subsubsection{Attention}

asdasd\footnote{
    Vaswani et al.: Attention is All You Need, S. 4
    \cite{vaswani2023attentionneed}
}:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}
    \left (
        \frac {QK^T} {\sqrt{d_k}}
    \right ) V
\end{equation}

\begin{equation}
    \text{softmax}(z) = \frac{e^z}{\sum_{j=1}^K e^{z_j}}
\end{equation}

% DDPM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diffusionmodelle}

Wie in vorherigen Abschnitten bereits erläutert, sind Log-Likelihood basierte Modelle in strenge mathematische Bedingungen gebunden. Dies schränkt ihre mögliche Komplexität ein. Modelle welche andere Kriterien optimieren, wie GANs erlauben zwar höhere Komplexität in ihren Funktionen, haben allerdings praktische Fallstricke wie das Mode-Collapse Problem. Somit stellt sich die Frage, ob es eine Klasse von Modellen gibt, welche die Log-Likelihood als Optimierungsziel haben und gleichzeitig beliebig komplexe Funktionen erlauben. \\
Eine Antwort hierauf stellen Diffusions Modelle (DM)\footnote{
    Dickstein et al.: Diffusion Models
    \cite{pmlr-v37-sohl-dickstein15}
}. Die Kernidee der Generierung dieser Klasse von Modellen ist dabei das, über viele Zeitschritte hinweg, iterative Herauskristallisieren von Informationen aus einem ursprünglichen Rauschbild. Intuitiv kann dies damit begründet werden, dass jeder einzelne Schritt hierbei eine Verhältnismäßig einfache Aufgabe ist, da das Ergebnis jedes Schrittes nur eine kleine Änderung des verrauschten Ausgangszustands darstellt. Alle Schritte zusammen jedoch ergeben ein sehr tiefes und mächtiges Modell. Vergleichbar ist dies mit einem Künstler welcher, beginnend mit einer Grundierung, Schicht für Schicht weitere Strukturen und Details auf eine Leinwand aufträgt. \\
Die Formulierung eines Diffusionsmodells ist in zwei Prozesse aufgeteilt, wobei $t$ den jeweiligen Zeitschritt angibt: 
\begin{itemize}
    \item \textbf{Vorwärtsprozess} $q(x_{1:T}|x_0)$: \\
    Das Ziel des Vorwärtsprozesses ist das iterative Einführen von zunehmenden, in der Regel gaußschen Rauschen in die Eingabedaten $x_0$. Der letzte Zeitschritt $x_T$ stellt hierbei ein komplett verrauschtes Bild dar, in welchem die Informationen von $x_0$ gänzlich zerstört wurden. Dieser Prozess ist der Grund für die Namensgebung von Diffusionsmodellen, da er dem Vorgang der Diffusion in der Physik ähnelt. Definiert ist er wie Folgt:
    \begin{equation}
        q(x_{1:T}|x_0) := \prod_{t=1}^T q(x_t | x_{t-1}) 
    \end{equation}
    \begin{equation}
        q(x_t|x_{t-1}) :=  \mathcal N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)
    \end{equation}
    $\beta_t$ ist dabei ein zeitabhängiger Wert, welcher die Stärke des gaußschen Rauschens des jeweiligen Zeitschritts bestimmt. Hierfür existieren mehrere Strategien, wobei eine einfache, positive lineare Funktion in Abhängigkeit von $t$ bereits ausreicht. Diese Strategien werden auch Noise-Schedules genannt. Ein hervorzuhebenes Detail ist, dass $q$ vollständig definiert ist und somit keinerlei zu trainierende Parameter enthält. \\
    $q(x_{1:T}|x_0)$ lässt sich zudem noch soweit vereinfachen, dass jedes $q(x_t|x_0)$ in nur einem einzigen Zeitschritt ermitteln lässt\footnote{
        Ho, Jain, Abbeel: Denoising Diffusion Probabilistic Models, S. 2
        \cite{ho2020denoisingdiffusionprobabilisticmodels}
    }:
    \begin{equation}
        q(x_t|x_0) :=  
        \mathcal N(x_t; \sqrt{1-\bar\alpha_t}x_0, (1 - \bar \alpha_t) I)
    \end{equation}
    Mit $\alpha_t := 1 - \beta_t$ und $\bar\alpha_t := \prod_{s=1}^t \alpha_s$.

    \item \textbf{Rückwärtsprozess} $p_\theta(x_{0:T})$: \\
    Der Rückwärtsprozess versucht, dem Namen entsprechend, die im Vorwärtsprozess getätigten Diffusionsschritte umzukehren - also aus dem Rauschen $x_T$ die Ursprungsdaten $x_0$ iterativ zu rekonstruieren. Entsprechend ist auch die mathematische Formulierung:
    \begin{equation}
        p_\theta(x_{0:T}) := p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} | x_{t}) 
    \end{equation}
    \begin{equation}
        p_\theta(x_{t-1} | x_{t})  :=  
        \mathcal N(x_{t-1}; \mu_\theta(x_{t}, t), \Sigma_\theta(x_{t}, t))
    \end{equation}
    Die Parameter des Rauschens $\mu$ und $\Sigma$ werden hierbei in Abhähnigkeit zum aktuellen und dem Ergebnis des jeweils vorherigen Zeitschritts durch neuronale Netze $\mu_\theta$ und $\Sigma_\theta$ abgebildet. Hierbei kommen nun erlernte Parameter $\theta$ zum Einsatz, da die Rekonstruktion eine deutlich kompliziertere Funktion darstellt, als die Diffusion. Ausgenommen ist hierbei $p(x_T)$, da dies ein vollständiges Rauschbild ist und somit $\mathcal N$ entspricht, also keine Parameter benötigt.
\end{itemize}
Die Notation dieser Prozesse ähnelt den Komponenten eines Autoencoder. Tatsächlich können Diffusionsmodelle als Variational Autoencoder verstanden werden, bei welchen die latente Abbildung einer Standardnormalverteilung entspricht\footnote{
    Ho, Jain, Abbeel: Denoising Diffusion Probabilistic Models, S. 3
    \cite{ho2020denoisingdiffusionprobabilisticmodels}
}.
Dementsprechend vergleichbar ist ebenfalls das Optimierungsziel, welches eine untere Grenze der Log-Likelihood, auch Variational Lower Bound (VLB) genannt, ist. Diese setzt sich aus drei grundlegenden Thermen, welche in Abhängigkeit zu $t$ stehen, zusammen\footnote{
    Dhariwal: Nichol, Diffusion Models Beat Gans on Image Synthesis, S. 19
    \cite{dhariwal2021diffusionmodelsbeatgans}
}:
\begin{equation}
    \min L_\text{vlb} := L_0 + L_{1:T-1} + L_T 
\end{equation}
Wobei die einzelnen hergeleiteten Therme hierbei folgend definiert sind:
\begin{equation}
    L_0 := -\log p_\theta(x_0|x_1)
\end{equation}
\begin{equation}
    L_{1:T-1} := D_\text{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1},x_t))
\end{equation}
\begin{equation}
    L_T := D_\text{KL}(q(x_T|x_0)||p(x_T))
\end{equation}
Diese Definition erscheint auf den ersten Blick nur schwer verständlich, bei genauerer Betrachtung lässt sich die Bedeutung relativ einfach zusammenfassen. Alle drei bilden jeweils die Distanz der jeweiligen Vorwärts- und Rückwärts-Verteilungen ab. Im allerersten Schritt, also $t=0$, sind die Eingabedaten noch unverrauscht. Somit müssen hier noch nicht die Normalverteilungen verglichen werden sondern lediglich die negative Log-Likelihood. Im letzten Zeitschritt werden reine Normalverteilungen verglichen, es benötigt also keine Parametrisierung $\theta$. Für alle schritte dazwischen müssen die jeweiligen Normalverteilungen mittels KL-Divergenz verglichen werden. Die zusätzliche Konditionierung des Vorwärtsprozesses durch $x_0$ ist hierbei notwendig, damit die Berechnung in polynomialer Zeit durchgeführt werden kann\footnote{
    Ho, Jain, Abbeel: Denoising Diffusion Probabilistic Models, S. 3
    \cite{ho2020denoisingdiffusionprobabilisticmodels}
}. 

\subsubsection{Denoising Diffusion Probabilistic Models}

Die vorherige Definition von Diffusionsmodellen im Allgemeinen bietet bereits ein Verständnis für die Leistungsfähigkeit dieser Klasse von Modellen. Das Prinzip des iterativen Erkennens von Strukturen in Rauschen ist bereits intuitiv leicht zu greifen. Eine besondere Formulierung von Diffusionsmodellen - Denoising Diffusion Probabilistic Models\footnote{
    Ho, Jain, Abbeel: Denoising Diffusion Probabilistic Models
    \cite{ho2020denoisingdiffusionprobabilisticmodels}
} (DDPMs) - macht die Begründung des Rauschens noch leichter zu erfassen. \\
Die grundlegende Umformulierung ist nun, dass anstelle des Erkennens der Struktur in einem Rauschbild, das Rauschen in einer verrauschten Struktur erkannt und iterativ entfernt wird. Auf den ersten Blick mag bereits ersichtlich sein, dass diese zwei Herangehensweisen grundsätzlich äquivalent sind, dieser Perspektivwechsel erlaubt allerdings neue Erkenntnisse. \\
Zuerst wird auf diese Weise der Zusammenhang zu sogenannten \textit{Score Matching Modellen} (SMM) offensichtlich. Diese Klasse von generativen Modellen erlernt die Datenverteilung nicht direkt, sondern ihren Score, also den Gradienten der Log-Likelihood. Auf diese Weise werden Eingabedaten also auf einen Vektor abgebildet, welche anzeigt wie die Daten verändert werden müssten, damit sie unter der Ursprungsverteilung wahrscheinlicher werden. Somit wird also indirekt ebenfalls die Datenverteilung erlernt. Der Samplingprozess unter einem solchen Modell ist das iterative Anpassen eines ursprünglichen Verrauschten Vektors, zu einem Sample, welches eine hohe Log-Likelihood unter der erlernten Verteilung hat. Dieser Prozess entspricht exakt nun exakt dem Rückwärtsprozess eines Diffusionsmodells. Nun kann die Frage aufkommen, wie wird in einem Diffusionsmodell nun aber der Score erlernt, wenn lediglich die Normalverteilungen des Vor- und Rückwärtsprozess angeglichen werden. Hier ist nun der Perspektivwechsel zum Erkennen des Rauschens relevant. Zu erlernen welches Rauschen in Daten vorhanden ist, ist äquivalent zum Erlernen des Scores. Um dies zu begreifen, muss sich vor Auge geführt werden, dass das Verrauschen eines einzelnen Datenpunktes bedeutet, dass er von seinem ursprünglichen Wert verschoben wird. Rauschen zu entfernen bedeutet somit, zu erlernen in welche Richtung und um wie viel ein Wert verschoben wurde. Mit anderen Worten also: Den Gradienten zu erlernen, welcher zum maximal wahrscheinlichen Ausgangswert weist. Was eben genau die Definition des Scores ist. Von Vorteil bei dieser Art von Modellen ist, dass sie gleichzeitig die tatsächliche Datenverteilung erlernern, wenn auch indirekt, und trotzdem nicht an die Invarianten einer WDF gebunden sind, da sie eben nur den Score erlernen. \\
Weiter kann so, unter der Annahme, dass die Kovarianzmatrix der Normalverteilungen konstant $\beta_t$ ist (Diese Annahme wird im folgenden Unterabschnitt kritisch hinterfragt), ein neues, deutlich simpleres Optimierungsziel formuliert werden\footnote{
    Ho, Jain, Abbeel: Denoising Diffusion Probabilistic Models, S. 5
    \cite{ho2020denoisingdiffusionprobabilisticmodels}
}: 
\begin{equation}
    \min L_\text{simple} = \mathbb E_{\epsilon \sim \mathcal N}
    \left [
        \| \epsilon - \epsilon_\theta(x_{t}, t) \|^2
    \right ]
\end{equation}
Hierbei erlernt das neurnale Netz $\epsilon_\theta$ nun nicht mehr Erwahrtungswert und Standardverteilung der zugrundeliegenden Normalverteilung, sondern das Rauschen $\epsilon$ selbst. 

\subsubsection{Improved DDPMs}

In der Praxis hat sich die Annahme, dass die Kovarianzmatrizen Normalverteilungen konstant sind als zu vereinfachend herausgestellt. Bessere Ergebnisse können durch das Erlernen von $\Sigma$ erzielt werden. Allerdings erhöht dies in der Praxis die Varianz der Trainingsverluste. Eine erfolgreiche Lösung wird Nichol und Dhariwal\footnote{
    Nichol, Dhariwal: Improved Denoising Diffusion Probabilistic Models, S. 4
    \cite{nichol2021improveddenoisingdiffusionprobabilistic}
} vorgeschlagen. Sie kombinieren die Optimierungsziele $L_\text{simple}$ und $L_\text{vlb}$:
\begin{equation}
    L_\text{hybrid} := L_\text{simple} + \lambda L_\text{vlb}
\end{equation}
$\lambda$ ist hierbei ein Gewicht, welches dafür sorgen soll, dass $L_\text{vlb}$ $L_\text{simple}$ nicht überwältigt. Ein wichtiges Detail ist, dass der valide Bereich für $\Sigma$ sehr klein ist. Deshalb wird das Ergebnis $v$ des neuronalen Netzes für $\Sigma$ zunächst noch durch $\beta_t$ und $\tilde \beta_t$ begrenzt:
\begin{equation}
    \Sigma_\theta(x_t, t) = e^{v \log \beta_t + (1 - v) \log \tilde \beta_t}
\end{equation}
\begin{equation}
    \tilde \beta_t = \frac{1-\bar \alpha_{t-1}} {1-\bar \alpha_{t}} \beta_t
\end{equation}
Die Autoren schlagen ebenfalls eine neue nicht-lineare Noise-Schedule vor welche auf dem Cosinus basiert: 
\begin{equation}
    \beta_t = \cos \left ( 
        \frac{t/T+s}{1+s} \frac{\pi}{2}
    \right )^2
\end{equation}
Diese soll dafür sorgen, dass Informationen in eingangsdaten langsamer zerstört werden als bei einer linearen Funktion und somit in jeder Trainingsschritt aussagekräftiger wird. $s$ ist von den Autoren auf 0.008 belegt worden.

\subsubsection{Classifier Free Guidance}

\subsubsection{Adaptive Group Normalization}

\subsubsection{Inpainting}


\subsubsection{Diffusion Transformer}


% LDM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Latent Diffusion}

Die bisher vorgestellte Variante von Diffusions-Modellen operiert ausschließlich in der Bilddomäne. Dies ist insbesonder bei hohen Auflösungen enorm Rechenintensiv, sowohl beim Training als auch bei der Inferenz. Latent Diffusion Modelle (LDM)\footnote{
    Vgl. Rombach et al.: Latent Diffusion Models
    \cite{rombach2022high}
}  sind ein Ansatz um dieses Problem zu mitigieren. \\
Grundlegend bestehen LDMs aus zwei Komponenten: Ein, auf den für das DM gedachten Trainingsdaten, vortrainierter VAE, sowie ein DM, wie es in bereits 2.2.3 beschrieben wurde. \\
Die Rolle des VAEs hierbei ist die Eingangsdaten in ihrer dimensionalität zu Reduzieren, sodass das DM auf signifikant weniger Dimensionen operiert. Da der nötige Rechenaufwand des DMs in aller Regel, insbesondere durch die sequenzielle Verarbeitung während der Inferenz, deutlich höher ist als die Ver- und Entschlüsselung durch einen VAE reduziert dies den Aufwand enorm.  \footnote{
    Vgl. Peebles, Xie: Diffusion Transformers, S. 8
    \cite{peebles2023scalable}
} \\
Die grundlegenden Algorithmen für Training und Inferenz bleiben hierbei weitestgehend unverändert. Beim Training müssen die Trainingsdaten nur zunächst kodiert werden und können dann genutzt werden. Bei der Inferenz muss lediglich das Resultat des Reverse-Prozesses einmal entschlüsselt werden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PTG
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Terrain-Generierung}

Terrain-Generierung ist ein zentrales Gebiet der Computergraphik. 

\subsection{Digital Elevation Models}

\textit{Digital Elevation Models} (DEMs) sind eine Datenstruktur für das Repräsentation von Höhenwerten. In ihnen werden beispielsweise Terrains somit über ihr Oberfläche definiert. Durch die Abbildung einer solchen dreidimensionalen Struktur auf zwei Dimensionen haben DEMs zwar den Nachteil, dass keine überhängenden Strukturen abgebildet werden können, da diese allerdings bei Landschaften nur sehr selten vorkommen, ist dies, insbesondere größeren Gebieten oft zu vernachlässigen.\\
Eine Variante von DEMs, welche die Höhenwerte den koordinaten eines zweidimensionalen Rasters zuordnet, wird im Bereich der Computergraphik auch oft als \textit{Heightmap} bezeichnet. Diese, sehr simple Datenstruktur, ist einfach und effizient zu Verarbeiten, weswegen sie oft in der Terraingenerierung angewandt wird. Aufgrund dieser Repräsentation als zweidimensionales Array sind DEMs insbesondere für die Verwendung mit Bildsynthese-Ansätzen der generativen KI geeigenet.

\subsection{Rauschbasierte Generierung}

In einigen Anwendungsgebieten, wie beispielsweise einer Terrain-Generierung zur Laufzeit eines Videospiels, ist es notwendig, dass die Algorithmen für die Erstellung der Landschaft so einfach wie möglich zu berechnen sind. Komplizierte Simulationen sind somit nicht Anwendbar. \\ 
Eine weit verbreitete alternative ist die Nutzung von sogennanten Rauschfunktionen. Solche Funktionen bilden ihre Parameter auf Zufallswerte, beziehungsweise Pseudozufallswerte ab. Angewandt auf DEMs sind diese Rauschfunktionen zweidimensional und bilden die $x$ und $y$ Koordinaten des Bildes auf einen zufälligen Höhenwert ab. \\ 
Reines statistisches Rauschen, welches beispielsweise auf einer Normalverteilung basiert, ist jedoch zu unstrukturiert um überzeugende Terrains erzeugen zu können - es fehlen größere Strukturen und Übergänge. Aus eben diesem Grund verwendet man häufig für Landschaften eine besondere Klasse von Rauschen - sogenannte \textit{Gitterrauschfunktionen}. Diese weisen Zufallswerte nun nicht mehr jedem einzelnen Bildpunkt zu. Stattdessen unterteilen sie das Bild zunächst in ein Gitter und bilden anschließend lediglich die Eckpunkte dieses Gitters zufällig ab. Nun kann zwischen diesen Werten interpoliert werden, um ein kohärenteres Zufallsbild zu erzeugen.\footnote{
    Vgl. Lagae et al.: A Survey of Procedural Noise Functions, S. 4
    \cite{https://doi.org/10.1111/j.1467-8659.2010.01827.x}
}  

\subsubsection{Perlin Rauschen}

Eine Verschärfung des Gitterrauschens ist das Gittergradientenrauschen. Hierbei werden jedem Eckpunkt des Gitters Zufallsgradienten, anstelle von Zufallswerten, zugeordnet. Die wohl populärste Methode dieser Kategorie ist das sogenannte Perlin Rauschen\footnote{
    Perlin: An Image Synthesizer
    \cite{perlin1985image}
}. 

\subsubsection{Fraktales Perlin Rauschen}

Sed lacinia fermentum odio quis faucibus. Phasellus blandit orci vitae ipsum rutrum aliquam. 

\subsection{KI-Basierte Generierung}

Fusce ipsum nisl, luctus in interdum non, sodales sed lacus. 

\subsubsection{Generierung mit GAN}

Curabitur tincidunt mauris ac venenatis accumsan. 

\subsubsection{Generierung mit Diffusion}

Mauris efficitur sit amet mauris in sodales. 

\subsubsection{Unter-Unterabschnitt\#3}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Geomorphologie
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Geomorphologie}


\subsection{Terrain Klassifizierung}

\subsection{Klima Klassifizierung}

